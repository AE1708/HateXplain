{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785d60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter, defaultdict\n",
    "from experiments import HateXplainExperiments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e858d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb__alpha': 0.01, 'nb__class_prior': None, 'nb__norm': False, 'sdg__alpha': 0.0001, 'sdg__average': True, 'sdg__class_weight': {0: 1.085606519835657, 1: 0.8211319985735391, 2: 1.1614065427295}, 'sdg__learning_rate': 'optimal', 'sdg__max_iter': 1000}\n",
      "{'Description': ['Exponential function applied to tokens in the TF-IDF vectorised training matrix (1) tagged as not indicative in offensive and hate-speech related human rationales and (2) and not unique to sentences labelled as normal [b]'], 'CV score': [0.6443101459808495], 'Accuracy': [0.6461538461538462], 'Balanced accuracy': [0.6210725722935097], 'AUC-ROC score': [0.8026349808176313], 'F1 score': [0.6291350326357898], 'Precision': [0.6412326891866073], 'Recall': [0.6461538461538462]}\n"
     ]
    }
   ],
   "source": [
    "f = open('dataset.json')\n",
    "data = json.load(f)\n",
    "\n",
    "vectorizer = pickle.load(open('models/vectorizer.pkl', 'rb'))\n",
    "\n",
    "description = \"Exponential function applied to tokens in the TF-IDF vectorised training matrix (1) tagged as not indicative in offensive and hate-speech related human rationales and (2) and not unique to sentences labelled as normal [b]\"\n",
    "hatexplain = HateXplainExperiments(description)\n",
    "dataset = hatexplain.prepare_dataset()\n",
    "hatexplain.prepare_properties(dataset)\n",
    "X_train_vect_experiment_B = hatexplain.preprocess_training_data_option_two()\n",
    "best_algo = hatexplain.get_performance_metrics(X_train_vect_experiment_B)\n",
    "\n",
    "# description = \"No injection\"\n",
    "# hatexplain = HateXplainExperiments(description)\n",
    "# dataset = hatexplain.prepare_dataset()\n",
    "# X_train_vect_no_injection = hatexplain.prepare_properties(dataset)\n",
    "# best_algo = hatexplain.get_performance_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c21992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = []\n",
    "\n",
    "for k,v in data.items():\n",
    "    \n",
    "    individual_data = {}\n",
    "    individual_data['post_id'] = v['post_id']\n",
    "    individual_data['post_tokens'] = v['post_tokens']\n",
    "    individual_data['post_tokens_joined'] = ' '.join(v['post_tokens'])\n",
    "    individual_data['target1'] = v['annotators'][0]['target']\n",
    "    individual_data['target2'] = v['annotators'][1]['target']\n",
    "    individual_data['target3'] = v['annotators'][2]['target']\n",
    "    individual_data['annotator_a_label'] = v['annotators'][0]['label']\n",
    "    individual_data['annotator_b_label'] = v['annotators'][1]['label']\n",
    "    individual_data['annotator_c_label'] = v['annotators'][2]['label']\n",
    "    \n",
    "    labels = [v['annotators'][0]['label'], v['annotators'][1]['label'], v['annotators'][2]['label']]\n",
    "    final_label = max(labels, key=labels.count)\n",
    "    individual_data['final_label'] = final_label\n",
    "    \n",
    "    if set(labels) != 3:\n",
    "    \n",
    "        if len(v['rationales']) == 0: \n",
    "            individual_data['annotator_a_rationale'] = []\n",
    "            individual_data['annotator_b_rationale'] = []\n",
    "            individual_data['annotator_c_rationale'] = []\n",
    "            individual_data['final_rationale'] = []\n",
    "            \n",
    "        else:  \n",
    "            individual_data['annotator_a_rationale'] = v['rationales'][0]\n",
    "            individual_data['annotator_b_rationale'] = v['rationales'][1]\n",
    "        \n",
    "            if final_label != v['annotators'][1]['label']:\n",
    "                individual_data['final_rationale'] = v['rationales'][0]\n",
    "            else:\n",
    "                individual_data['final_rationale'] = v['rationales'][1]\n",
    "                       \n",
    "        cleaned_dataset.append(individual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1684e7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>post_tokens_joined</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>annotator_a_label</th>\n",
       "      <th>annotator_b_label</th>\n",
       "      <th>annotator_c_label</th>\n",
       "      <th>final_label</th>\n",
       "      <th>annotator_a_rationale</th>\n",
       "      <th>annotator_b_rationale</th>\n",
       "      <th>annotator_c_rationale</th>\n",
       "      <th>final_rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[African]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[African]</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>offensive</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>[Caucasian, Women]</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>offensive</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       post_id   \n",
       "0  1179055004553900032_twitter  \\\n",
       "1  1179063826874032128_twitter   \n",
       "2  1178793830532956161_twitter   \n",
       "3  1179088797964763136_twitter   \n",
       "4  1179085312976445440_twitter   \n",
       "\n",
       "                                         post_tokens   \n",
       "0  [i, dont, think, im, getting, my, baby, them, ...  \\\n",
       "1  [we, cannot, continue, calling, ourselves, fem...   \n",
       "2                [nawt, yall, niggers, ignoring, me]   \n",
       "3  [<user>, i, am, bit, confused, coz, chinese, p...   \n",
       "4  [this, bitch, in, whataburger, eating, a, burg...   \n",
       "\n",
       "                                  post_tokens_joined             target1   \n",
       "0  i dont think im getting my baby them white 9 h...              [None]  \\\n",
       "1  we cannot continue calling ourselves feminists...              [None]   \n",
       "2                      nawt yall niggers ignoring me           [African]   \n",
       "3  <user> i am bit confused coz chinese ppl can n...             [Asian]   \n",
       "4  this bitch in whataburger eating a burger with...  [Caucasian, Women]   \n",
       "\n",
       "              target2             target3 annotator_a_label annotator_b_label   \n",
       "0              [None]              [None]            normal            normal  \\\n",
       "1              [None]              [None]            normal            normal   \n",
       "2              [None]           [African]            normal            normal   \n",
       "3             [Asian]             [Asian]        hatespeech         offensive   \n",
       "4  [Women, Caucasian]  [Women, Caucasian]        hatespeech        hatespeech   \n",
       "\n",
       "  annotator_c_label final_label   \n",
       "0            normal      normal  \\\n",
       "1            normal      normal   \n",
       "2        hatespeech      normal   \n",
       "3        hatespeech  hatespeech   \n",
       "4         offensive  hatespeech   \n",
       "\n",
       "                               annotator_a_rationale   \n",
       "0                                                 []  \\\n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               annotator_b_rationale annotator_c_rationale   \n",
       "0                                                 []                    []  \\\n",
       "1                                                 []                    []   \n",
       "2                                                 []                    []   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                   NaN   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                   NaN   \n",
       "\n",
       "                                     final_rationale  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cleaned_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1615469",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(df.index, test_size=0.2, random_state=42)\n",
    "df_train = df.loc[train_indices]\n",
    "df_test = df.loc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb623f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPSN (Background Positive, Subgroup Negative) AUC\n",
    "# BNSP (Background Negative, Subgroup Positive) AUC\n",
    "# GMB-Subgroup-AUC\n",
    "# GMB-BPSN-AUC\n",
    "# GMB-BNSP-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45eb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/hate-alert/HateXplain/blob/master/Bias_Calculation_NB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7506a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_information(dataset):\n",
    "    \n",
    "    final_target_output = defaultdict(list)\n",
    "    all_communities_selected = []\n",
    "    \n",
    "    for each in dataset.iterrows(): \n",
    "        \n",
    "        # All the target communities tagged for this post\n",
    "        all_targets = each[1]['target1'] + each[1]['target2'] + each[1]['target3']  \n",
    "        community_dict = dict(Counter(all_targets))\n",
    "        \n",
    "        # Select only those communities which are present more than once.\n",
    "        for key in community_dict:\n",
    "            if community_dict[key]>1:  \n",
    "                final_target_output[each[1]['post_id']].append(key)\n",
    "                all_communities_selected.append(key)\n",
    "        \n",
    "        # If no community is selected based on majority voting then we don't select any community\n",
    "        if each[1]['post_id'] not in final_target_output:\n",
    "            final_target_output[each[1]['post_id']].append('None')\n",
    "            all_communities_selected.append(key)\n",
    "\n",
    "    return final_target_output, all_communities_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562e7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_information, all_communities_selected = generate_target_information(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5f57bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['African',\n",
       " 'Islam',\n",
       " 'Jewish',\n",
       " 'Homosexual',\n",
       " 'Women',\n",
       " 'Refugee',\n",
       " 'Arab',\n",
       " 'Caucasian',\n",
       " 'Asian',\n",
       " 'Hispanic']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_count_dict = Counter(all_communities_selected)\n",
    "\n",
    "# We remove None and Other from dictionary\n",
    "community_count_dict.pop('None')\n",
    "community_count_dict.pop('Other')\n",
    "\n",
    "# For the bias calculation, we are considering the top 10 communites based on their count\n",
    "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
    "list_selected_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a648fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_target_information = {}\n",
    "for each in target_information:\n",
    "    temp = list(set(target_information[each]) & set(list_selected_community))\n",
    "    if len(temp) == 0:\n",
    "        final_target_information[each] = None\n",
    "    else:\n",
    "        final_target_information[each] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c463f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['final_target_category'] = df_test['post_id'].map(final_target_information) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c711e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bias methods that will be considered\n",
    "\n",
    "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
    "community_list = list(list_selected_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0867a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_vect = vectorizer.transform(df_test['post_tokens_joined'])\n",
    "df_test['classification'] = best_algo.predict(sentences_vect)\n",
    "\n",
    "df_test['final_label'] = df_test['final_label'].replace({'offensive': 'toxic', 'hatespeech': 'toxic', 'normal': 'non-toxic'})\n",
    "df_test['classification'] = df_test['classification'].replace({'offensive': 'toxic', 'hatespeech': 'toxic', 'normal': 'non-toxic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8745803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide the ids into postive or class class based on the method.\n",
    "def bias_evaluation_metric(dataset, method, community):\n",
    "    \n",
    "    positive_ids = []\n",
    "    negative_ids = []\n",
    "    \n",
    "    if method=='subgroup':\n",
    "        for eachrow in dataset.iterrows():\n",
    "            if eachrow[1]['final_target_category'] == None:\n",
    "                continue\n",
    "            if community in eachrow[1]['final_target_category']:\n",
    "                if eachrow[1]['final_label'] =='non-toxic':\n",
    "                    negative_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    positive_ids.append(eachrow[1]['post_id'])\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    elif method=='bpsn':\n",
    "        for eachrow in dataset.iterrows():\n",
    "            if eachrow[1]['final_target_category'] == None:\n",
    "                continue\n",
    "            if community in eachrow[1]['final_target_category']:\n",
    "                if eachrow[1]['final_label'] =='non-toxic':\n",
    "                    negative_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if eachrow[1]['final_label'] !='non-toxic':\n",
    "                    positive_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    elif method=='bnsp':\n",
    "        for eachrow in dataset.iterrows():\n",
    "            if eachrow[1]['final_target_category'] == None:\n",
    "                continue\n",
    "            if community in eachrow[1]['final_target_category']:\n",
    "                if eachrow[1]['final_label'] !='non-toxic':\n",
    "                    positive_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if eachrow[1]['final_label'] =='non-toxic':\n",
    "                    negative_ids.append(eachrow[1]['post_id'])\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810139b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "# We compute the bias score using each method for all the community\n",
    "for each_method in method_list:\n",
    "    for each_community in community_list:\n",
    "        community_data = bias_evaluation_metric(df_test, each_method, each_community)\n",
    "        \n",
    "        truth_values = []\n",
    "        prediction_values = []\n",
    "\n",
    "        label_to_value = {'toxic': 1.0, 'non-toxic': 0.0}\n",
    "        \n",
    "        for each in community_data['positiveID']:\n",
    "            truth_values.append(label_to_value[df_test[df_test['post_id'] == each]['final_label'].iloc[0]])\n",
    "            prediction_values.append(label_to_value[df_test[df_test['post_id'] == each]['classification'].iloc[0]])\n",
    "\n",
    "        for each in community_data['negativeID']:\n",
    "            truth_values.append(label_to_value[df_test[df_test['post_id'] == each]['final_label'].iloc[0]])\n",
    "            prediction_values.append(label_to_value[df_test[df_test['post_id'] == each]['classification'].iloc[0]])\n",
    "\n",
    "        roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
    "        final_bias_dictionary[each_method][each_community] = roc_output_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1208c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subgroup 0.6712524178525073\n",
      "bpsn 0.650849979794217\n",
      "bnsp 0.6487227864376864\n"
     ]
    }
   ],
   "source": [
    "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "power_value = -5\n",
    "num_communities = len(community_list)\n",
    "\n",
    "\n",
    "for each_method in final_bias_dictionary:\n",
    "    temp_value =[]\n",
    "    for each_community in final_bias_dictionary[each_method]:\n",
    "        temp_value.append(pow(final_bias_dictionary[each_method][each_community], power_value))\n",
    "    print(each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f59211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aix360",
   "language": "python",
   "name": "aix360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
